    61/50000: episode: 1, duration: 4.258s, episode steps: 61, steps per second: 14, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.032 [-1.543, 1.146], loss: 0.426877, mean_absolute_error: 0.502965, mean_q: 0.178553\
    86/50000: episode: 2, duration: 0.414s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.040 [-0.914, 0.635], loss: 0.197014, mean_absolute_error: 0.543120, mean_q: 0.681694\
    99/50000: episode: 3, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.070 [-1.538, 1.029], loss: 0.086104, mean_absolute_error: 0.641244, mean_q: 1.058704\
   118/50000: episode: 4, duration: 0.317s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.051 [-2.817, 1.923], loss: 0.059240, mean_absolute_error: 0.656439, mean_q: 1.186122\
   129/50000: episode: 5, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.115 [-2.036, 1.218], loss: 0.041494, mean_absolute_error: 0.678526, mean_q: 1.219883\
   148/50000: episode: 6, duration: 0.318s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.077 [-1.376, 0.788], loss: 0.023484, mean_absolute_error: 0.726019, mean_q: 1.366676\
   160/50000: episode: 7, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.124 [-1.328, 2.207], loss: 0.013899, mean_absolute_error: 0.780200, mean_q: 1.515933\
   176/50000: episode: 8, duration: 0.264s, episode steps: 16, steps per second: 61, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.076 [-2.546, 1.563], loss: 0.026036, mean_absolute_error: 0.836868, mean_q: 1.616979\
   190/50000: episode: 9, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.076 [-1.653, 1.029], loss: 0.022703, mean_absolute_error: 0.860906, mean_q: 1.673926\
   199/50000: episode: 10, duration: 0.149s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.123 [-1.194, 1.961], loss: 0.019575, mean_absolute_error: 0.898136, mean_q: 1.776885\
   229/50000: episode: 11, duration: 0.492s, episode steps: 30, steps per second: 61, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.633 [0.000, 1.000], mean observation: -0.005 [-2.431, 1.718], loss: 0.036534, mean_absolute_error: 0.977635, mean_q: 1.896798\
   253/50000: episode: 12, duration: 0.399s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.059 [-0.754, 1.503], loss: 0.025077, mean_absolute_error: 1.060407, mean_q: 2.097751\
   271/50000: episode: 13, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.091 [-1.747, 0.845], loss: 0.042139, mean_absolute_error: 1.150211, mean_q: 2.215672\
   284/50000: episode: 14, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.102 [-0.964, 1.591], loss: 0.044553, mean_absolute_error: 1.210659, mean_q: 2.337769\
   304/50000: episode: 15, duration: 0.334s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.594, 1.084], loss: 0.039436, mean_absolute_error: 1.260316, mean_q: 2.478751\
   326/50000: episode: 16, duration: 0.365s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.085 [-1.429, 0.983], loss: 0.044925, mean_absolute_error: 1.359421, mean_q: 2.655441\
   354/50000: episode: 17, duration: 0.467s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.082 [-1.699, 0.819], loss: 0.072668, mean_absolute_error: 1.467331, mean_q: 2.813634\
   368/50000: episode: 18, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.089 [-1.588, 0.970], loss: 0.085394, mean_absolute_error: 1.547437, mean_q: 2.979358\
   385/50000: episode: 19, duration: 0.282s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.063 [-1.266, 0.808], loss: 0.090620, mean_absolute_error: 1.621746, mean_q: 3.098412\
   402/50000: episode: 20, duration: 0.284s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.065 [-2.208, 1.376], loss: 0.095030, mean_absolute_error: 1.654079, mean_q: 3.104396\
   425/50000: episode: 21, duration: 0.383s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.116 [-0.590, 1.508], loss: 0.101981, mean_absolute_error: 1.753346, mean_q: 3.334682\
   458/50000: episode: 22, duration: 0.548s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: 0.062 [-2.394, 2.097], loss: 0.087250, mean_absolute_error: 1.863637, mean_q: 3.595175\
   477/50000: episode: 23, duration: 0.317s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.048 [-1.706, 1.206], loss: 0.112747, mean_absolute_error: 1.950301, mean_q: 3.735965\
   497/50000: episode: 24, duration: 0.332s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.061 [-1.653, 0.998], loss: 0.150241, mean_absolute_error: 2.055856, mean_q: 3.900519\
   529/50000: episode: 25, duration: 0.534s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.135 [-1.402, 0.989], loss: 0.127428, mean_absolute_error: 2.147351, mean_q: 4.111210\
   541/50000: episode: 26, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.106 [-1.167, 2.083], loss: 0.212073, mean_absolute_error: 2.229719, mean_q: 4.141238\
   573/50000: episode: 27, duration: 0.533s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: -0.103 [-1.919, 1.978], loss: 0.156883, mean_absolute_error: 2.328894, mean_q: 4.461095\
   591/50000: episode: 28, duration: 0.299s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.087 [-1.434, 0.745], loss: 0.131276, mean_absolute_error: 2.409210, mean_q: 4.661770\
   604/50000: episode: 29, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.075 [-1.572, 1.026], loss: 0.200326, mean_absolute_error: 2.472438, mean_q: 4.766522\
   636/50000: episode: 30, duration: 0.531s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.106 [-0.878, 0.629], loss: 0.162802, mean_absolute_error: 2.543193, mean_q: 4.921553\
   658/50000: episode: 31, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.125 [-0.886, 0.554], loss: 0.223467, mean_absolute_error: 2.682810, mean_q: 5.185645\
   710/50000: episode: 32, duration: 0.867s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.056 [-0.651, 1.384], loss: 0.239984, mean_absolute_error: 2.819685, mean_q: 5.464309\
   725/50000: episode: 33, duration: 0.250s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.075 [-1.259, 0.807], loss: 0.281100, mean_absolute_error: 2.952404, mean_q: 5.738527\
   766/50000: episode: 34, duration: 0.682s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.005 [-1.298, 1.700], loss: 0.264347, mean_absolute_error: 3.077096, mean_q: 5.975882\
   791/50000: episode: 35, duration: 0.416s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.108 [-0.446, 1.370], loss: 0.257099, mean_absolute_error: 3.218626, mean_q: 6.291272\
   809/50000: episode: 36, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.096 [-1.587, 0.776], loss: 0.228846, mean_absolute_error: 3.325323, mean_q: 6.514086\
   839/50000: episode: 37, duration: 0.500s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.044 [-1.348, 2.011], loss: 0.307544, mean_absolute_error: 3.439476, mean_q: 6.709396\
   883/50000: episode: 38, duration: 0.732s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-0.982, 0.611], loss: 0.439506, mean_absolute_error: 3.544398, mean_q: 6.848528\
   903/50000: episode: 39, duration: 0.332s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.087 [-1.544, 0.793], loss: 0.352662, mean_absolute_error: 3.643039, mean_q: 7.130189\
   950/50000: episode: 40, duration: 0.783s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.404 [0.000, 1.000], mean observation: -0.033 [-1.942, 2.486], loss: 0.373446, mean_absolute_error: 3.813032, mean_q: 7.483744\
   967/50000: episode: 41, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.102 [-0.784, 1.359], loss: 0.451654, mean_absolute_error: 3.966106, mean_q: 7.702027\
   984/50000: episode: 42, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.066 [-1.029, 1.607], loss: 0.331456, mean_absolute_error: 3.987871, mean_q: 7.798388\
  1003/50000: episode: 43, duration: 0.316s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.025 [-1.791, 1.212], loss: 0.527355, mean_absolute_error: 4.061750, mean_q: 7.880209\
  1054/50000: episode: 44, duration: 0.850s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.118 [-1.361, 0.622], loss: 0.456201, mean_absolute_error: 4.222223, mean_q: 8.265666\
  1083/50000: episode: 45, duration: 0.482s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.017 [-1.743, 1.211], loss: 0.532967, mean_absolute_error: 4.399671, mean_q: 8.571150\
  1180/50000: episode: 46, duration: 1.615s, episode steps: 97, steps per second: 60, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.278 [-2.566, 1.279], loss: 0.480507, mean_absolute_error: 4.600977, mean_q: 9.056850\
  1214/50000: episode: 47, duration: 0.567s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.031 [-1.694, 1.389], loss: 0.805907, mean_absolute_error: 4.945078, mean_q: 9.619282\
  1270/50000: episode: 48, duration: 0.933s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.079 [-1.080, 0.743], loss: 0.631546, mean_absolute_error: 5.100067, mean_q: 10.020606\
  1292/50000: episode: 49, duration: 0.366s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.435, 0.967], loss: 0.698576, mean_absolute_error: 5.230317, mean_q: 10.336129\
  1331/50000: episode: 50, duration: 0.649s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.049 [-1.118, 0.833], loss: 0.548262, mean_absolute_error: 5.303451, mean_q: 10.548687\
  1349/50000: episode: 51, duration: 0.298s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.095 [-1.311, 0.638], loss: 0.661714, mean_absolute_error: 5.445583, mean_q: 10.767272\
  1440/50000: episode: 52, duration: 1.518s, episode steps: 91, steps per second: 60, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.027 [-1.182, 1.250], loss: 0.532699, mean_absolute_error: 5.690063, mean_q: 11.430841\
  1482/50000: episode: 53, duration: 0.699s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.127 [-1.086, 0.410], loss: 0.679365, mean_absolute_error: 5.944150, mean_q: 11.889179\
  1537/50000: episode: 54, duration: 0.917s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.013 [-1.498, 1.336], loss: 0.787111, mean_absolute_error: 6.192994, mean_q: 12.311659\
  1624/50000: episode: 55, duration: 1.449s, episode steps: 87, steps per second: 60, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.068 [-1.409, 0.811], loss: 0.602668, mean_absolute_error: 6.394287, mean_q: 12.832239\
  1732/50000: episode: 56, duration: 1.799s, episode steps: 108, steps per second: 60, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.355 [-1.528, 0.968], loss: 0.699076, mean_absolute_error: 6.879092, mean_q: 13.852914\
  1867/50000: episode: 57, duration: 2.248s, episode steps: 135, steps per second: 60, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.220 [-1.471, 0.877], loss: 0.908498, mean_absolute_error: 7.420484, mean_q: 14.903431\
  1995/50000: episode: 58, duration: 2.134s, episode steps: 128, steps per second: 60, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.232 [-1.622, 1.007], loss: 0.888103, mean_absolute_error: 8.121478, mean_q: 16.404003\
  2164/50000: episode: 59, duration: 2.816s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.204 [-1.497, 0.847], loss: 1.132264, mean_absolute_error: 8.765150, mean_q: 17.662083\
  2309/50000: episode: 60, duration: 2.416s, episode steps: 145, steps per second: 60, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.198 [-1.175, 0.942], loss: 1.331359, mean_absolute_error: 9.475179, mean_q: 19.109900\
  2398/50000: episode: 61, duration: 1.481s, episode steps: 89, steps per second: 60, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.298 [-1.319, 0.825], loss: 1.110518, mean_absolute_error: 9.922545, mean_q: 20.110210\
  2514/50000: episode: 62, duration: 1.934s, episode steps: 116, steps per second: 60, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.283 [-1.640, 0.831], loss: 1.449585, mean_absolute_error: 10.419283, mean_q: 21.076466\
  2625/50000: episode: 63, duration: 1.849s, episode steps: 111, steps per second: 60, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.271 [-1.451, 0.773], loss: 1.788195, mean_absolute_error: 10.938271, mean_q: 22.056715\
  2750/50000: episode: 64, duration: 2.082s, episode steps: 125, steps per second: 60, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.247 [-1.425, 0.801], loss: 1.494182, mean_absolute_error: 11.388411, mean_q: 23.106470\
  2897/50000: episode: 65, duration: 2.448s, episode steps: 147, steps per second: 60, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.253 [-1.834, 0.885], loss: 1.907233, mean_absolute_error: 12.022844, mean_q: 24.370340\
  3083/50000: episode: 66, duration: 3.100s, episode steps: 186, steps per second: 60, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.243 [-2.179, 0.961], loss: 1.623419, mean_absolute_error: 12.750538, mean_q: 25.940666\
  3206/50000: episode: 67, duration: 1.189s, episode steps: 123, steps per second: 103, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.401 [-2.027, 0.825], loss: 1.695995, mean_absolute_error: 13.411528, mean_q: 27.238771\
  3406/50000: episode: 68, duration: 0.510s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.169 [-1.817, 0.969], loss: 1.815053, mean_absolute_error: 14.088437, mean_q: 28.633495\
  3545/50000: episode: 69, duration: 0.342s, episode steps: 139, steps per second: 406, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.283 [-1.630, 0.907], loss: 1.870480, mean_absolute_error: 14.772964, mean_q: 30.162096\
  3732/50000: episode: 70, duration: 0.511s, episode steps: 187, steps per second: 366, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.337 [-2.418, 0.924], loss: 1.736326, mean_absolute_error: 15.545474, mean_q: 31.702337\
  3878/50000: episode: 71, duration: 0.438s, episode steps: 146, steps per second: 333, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.184 [-0.792, 1.412], loss: 2.041698, mean_absolute_error: 16.302677, mean_q: 33.189621\
  4063/50000: episode: 72, duration: 0.472s, episode steps: 185, steps per second: 392, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.338 [-2.919, 0.921], loss: 2.168712, mean_absolute_error: 16.907907, mean_q: 34.434361\
  4213/50000: episode: 73, duration: 0.460s, episode steps: 150, steps per second: 326, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.405 [-2.319, 0.814], loss: 2.000382, mean_absolute_error: 17.600927, mean_q: 35.810757\
  4413/50000: episode: 74, duration: 0.540s, episode steps: 200, steps per second: 370, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.054 [-0.901, 0.815], loss: 1.880174, mean_absolute_error: 18.276691, mean_q: 37.318390\
  4602/50000: episode: 75, duration: 0.488s, episode steps: 189, steps per second: 387, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.325 [-2.551, 1.007], loss: 2.694594, mean_absolute_error: 19.177603, mean_q: 39.057442\
  4787/50000: episode: 76, duration: 0.443s, episode steps: 185, steps per second: 418, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.336 [-2.707, 0.674], loss: 2.563361, mean_absolute_error: 19.861584, mean_q: 40.392822\
  4949/50000: episode: 77, duration: 0.426s, episode steps: 162, steps per second: 380, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.375 [-2.929, 0.923], loss: 2.778711, mean_absolute_error: 20.449711, mean_q: 41.495098\
  5149/50000: episode: 78, duration: 0.514s, episode steps: 200, steps per second: 389, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.092 [-0.926, 1.352], loss: 2.798846, mean_absolute_error: 21.060410, mean_q: 42.718895\
  5309/50000: episode: 79, duration: 0.441s, episode steps: 160, steps per second: 363, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.390 [-2.416, 0.992], loss: 2.379261, mean_absolute_error: 21.742025, mean_q: 44.059921\
  5456/50000: episode: 80, duration: 0.341s, episode steps: 147, steps per second: 431, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.416 [-2.543, 0.882], loss: 1.961765, mean_absolute_error: 22.350145, mean_q: 45.338528\
  5655/50000: episode: 81, duration: 0.490s, episode steps: 199, steps per second: 406, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.321 [-2.736, 0.940], loss: 2.414418, mean_absolute_error: 22.842796, mean_q: 46.345345\
  5804/50000: episode: 82, duration: 0.349s, episode steps: 149, steps per second: 427, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.405 [-2.533, 0.642], loss: 3.430814, mean_absolute_error: 23.167814, mean_q: 46.913479\
  5961/50000: episode: 83, duration: 0.375s, episode steps: 157, steps per second: 419, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.388 [-2.602, 0.749], loss: 2.507335, mean_absolute_error: 23.593689, mean_q: 47.896248\
  6131/50000: episode: 84, duration: 0.443s, episode steps: 170, steps per second: 384, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.367 [-2.421, 0.728], loss: 2.629068, mean_absolute_error: 24.222101, mean_q: 49.219933\
  6331/50000: episode: 85, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.179 [-1.672, 1.034], loss: 2.059819, mean_absolute_error: 24.739565, mean_q: 50.268555\
  6497/50000: episode: 86, duration: 0.415s, episode steps: 166, steps per second: 400, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.366 [-2.575, 0.855], loss: 2.186255, mean_absolute_error: 25.395943, mean_q: 51.521767\
  6656/50000: episode: 87, duration: 0.385s, episode steps: 159, steps per second: 413, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.386 [-2.533, 0.784], loss: 2.276694, mean_absolute_error: 25.900291, mean_q: 52.498322\
  6801/50000: episode: 88, duration: 0.342s, episode steps: 145, steps per second: 424, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.418 [-2.420, 0.940], loss: 1.991706, mean_absolute_error: 26.413450, mean_q: 53.632812\
  6970/50000: episode: 89, duration: 0.399s, episode steps: 169, steps per second: 423, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.362 [-2.598, 0.827], loss: 3.399569, mean_absolute_error: 26.540644, mean_q: 53.805450\
  7117/50000: episode: 90, duration: 0.349s, episode steps: 147, steps per second: 422, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.398 [-2.540, 0.783], loss: 2.997755, mean_absolute_error: 27.040071, mean_q: 54.759003\
  7272/50000: episode: 91, duration: 0.363s, episode steps: 155, steps per second: 426, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.393 [-2.912, 1.117], loss: 2.743171, mean_absolute_error: 27.183279, mean_q: 55.072254\
  7452/50000: episode: 92, duration: 0.426s, episode steps: 180, steps per second: 423, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.341 [-2.425, 0.855], loss: 2.054471, mean_absolute_error: 27.884636, mean_q: 56.516460\
  7614/50000: episode: 93, duration: 0.390s, episode steps: 162, steps per second: 415, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.390 [-2.568, 0.830], loss: 3.024552, mean_absolute_error: 28.219072, mean_q: 57.075775\
  7776/50000: episode: 94, duration: 0.386s, episode steps: 162, steps per second: 420, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.384 [-2.596, 0.709], loss: 2.486881, mean_absolute_error: 28.555923, mean_q: 57.920639\
  7976/50000: episode: 95, duration: 0.478s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.083 [-0.803, 1.135], loss: 2.639579, mean_absolute_error: 29.137253, mean_q: 59.037811\
  8139/50000: episode: 96, duration: 0.383s, episode steps: 163, steps per second: 425, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.377 [-2.717, 0.801], loss: 3.201879, mean_absolute_error: 29.392590, mean_q: 59.391075\
  8281/50000: episode: 97, duration: 0.353s, episode steps: 142, steps per second: 403, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.424 [-2.423, 0.961], loss: 3.076881, mean_absolute_error: 29.847464, mean_q: 60.455410\
  8444/50000: episode: 98, duration: 0.421s, episode steps: 163, steps per second: 387, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.376 [-2.732, 0.807], loss: 3.486355, mean_absolute_error: 30.208595, mean_q: 61.198544\
  8624/50000: episode: 99, duration: 0.477s, episode steps: 180, steps per second: 377, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.336 [-2.591, 0.821], loss: 2.532157, mean_absolute_error: 30.697859, mean_q: 62.243011\
  8766/50000: episode: 100, duration: 0.351s, episode steps: 142, steps per second: 405, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.434 [-2.997, 1.004], loss: 2.740858, mean_absolute_error: 30.535191, mean_q: 61.891808\
  8913/50000: episode: 101, duration: 0.361s, episode steps: 147, steps per second: 407, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.415 [-2.414, 0.717], loss: 3.413217, mean_absolute_error: 30.648764, mean_q: 62.111401\
  9069/50000: episode: 102, duration: 0.385s, episode steps: 156, steps per second: 405, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.392 [-2.600, 0.933], loss: 2.934883, mean_absolute_error: 31.161715, mean_q: 62.975956\
  9235/50000: episode: 103, duration: 0.548s, episode steps: 166, steps per second: 303, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.372 [-2.576, 0.846], loss: 3.486603, mean_absolute_error: 31.578379, mean_q: 63.966774\
  9408/50000: episode: 104, duration: 0.577s, episode steps: 173, steps per second: 300, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.361 [-2.573, 0.795], loss: 3.196274, mean_absolute_error: 31.493570, mean_q: 63.740765\
  9573/50000: episode: 105, duration: 0.404s, episode steps: 165, steps per second: 408, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.372 [-2.974, 1.045], loss: 2.820010, mean_absolute_error: 32.101139, mean_q: 64.967590\
  9726/50000: episode: 106, duration: 0.378s, episode steps: 153, steps per second: 405, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.407 [-2.769, 0.762], loss: 3.592949, mean_absolute_error: 32.091976, mean_q: 64.896980\
  9926/50000: episode: 107, duration: 0.481s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.223 [-2.008, 0.784], loss: 2.884758, mean_absolute_error: 32.490921, mean_q: 65.663498\
 10108/50000: episode: 108, duration: 0.587s, episode steps: 182, steps per second: 310, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.341 [-2.544, 0.931], loss: 3.054287, mean_absolute_error: 32.897461, mean_q: 66.509026\
 10302/50000: episode: 109, duration: 0.463s, episode steps: 194, steps per second: 419, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.323 [-2.530, 0.725], loss: 3.126048, mean_absolute_error: 33.145519, mean_q: 66.973412\
 10462/50000: episode: 110, duration: 0.384s, episode steps: 160, steps per second: 417, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.379 [-2.587, 0.958], loss: 2.063495, mean_absolute_error: 33.517830, mean_q: 67.867905\
 10613/50000: episode: 111, duration: 0.417s, episode steps: 151, steps per second: 363, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.403 [-2.776, 0.827], loss: 3.129826, mean_absolute_error: 33.380516, mean_q: 67.636406\
 10772/50000: episode: 112, duration: 0.445s, episode steps: 159, steps per second: 357, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.385 [-2.563, 0.926], loss: 2.557469, mean_absolute_error: 33.596920, mean_q: 68.028587\
 10972/50000: episode: 113, duration: 0.472s, episode steps: 200, steps per second: 424, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.080 [-1.098, 0.770], loss: 3.317822, mean_absolute_error: 33.880966, mean_q: 68.594193\
 11132/50000: episode: 114, duration: 0.374s, episode steps: 160, steps per second: 428, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.380 [-2.806, 0.995], loss: 2.079669, mean_absolute_error: 34.108162, mean_q: 69.022430\
 11297/50000: episode: 115, duration: 0.390s, episode steps: 165, steps per second: 423, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.369 [-2.420, 0.684], loss: 3.884223, mean_absolute_error: 34.502823, mean_q: 69.707436\
 11497/50000: episode: 116, duration: 0.503s, episode steps: 200, steps per second: 398, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.282 [-2.615, 0.927], loss: 2.307721, mean_absolute_error: 34.552521, mean_q: 69.961884\
 11661/50000: episode: 117, duration: 0.430s, episode steps: 164, steps per second: 381, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.365 [-2.582, 0.963], loss: 3.844882, mean_absolute_error: 34.574345, mean_q: 69.909210\
 11809/50000: episode: 118, duration: 0.361s, episode steps: 148, steps per second: 410, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.403 [-2.427, 0.828], loss: 3.170640, mean_absolute_error: 34.689644, mean_q: 70.166153\
 11947/50000: episode: 119, duration: 0.343s, episode steps: 138, steps per second: 402, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.432 [-2.437, 0.887], loss: 3.020279, mean_absolute_error: 34.771957, mean_q: 70.359428\
 12096/50000: episode: 120, duration: 0.398s, episode steps: 149, steps per second: 374, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.405 [-2.549, 1.034], loss: 3.717957, mean_absolute_error: 34.852879, mean_q: 70.446289\
 12235/50000: episode: 121, duration: 0.372s, episode steps: 139, steps per second: 373, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.438 [-2.618, 0.794], loss: 3.492309, mean_absolute_error: 35.022514, mean_q: 70.810181\
 12435/50000: episode: 122, duration: 0.607s, episode steps: 200, steps per second: 330, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.246 [-2.219, 0.711], loss: 2.777390, mean_absolute_error: 35.415707, mean_q: 71.601852\
 12583/50000: episode: 123, duration: 0.483s, episode steps: 148, steps per second: 307, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.410 [-2.618, 0.728], loss: 2.470809, mean_absolute_error: 35.441189, mean_q: 71.662308\
 12761/50000: episode: 124, duration: 0.459s, episode steps: 178, steps per second: 388, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.337 [-2.777, 1.150], loss: 2.688529, mean_absolute_error: 35.590103, mean_q: 71.962646\
 12916/50000: episode: 125, duration: 0.426s, episode steps: 155, steps per second: 363, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.384 [-2.970, 1.251], loss: 2.434036, mean_absolute_error: 35.496040, mean_q: 71.812340\
 13057/50000: episode: 126, duration: 0.381s, episode steps: 141, steps per second: 370, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.415 [-2.561, 0.922], loss: 3.619656, mean_absolute_error: 35.604675, mean_q: 71.982605\
 13223/50000: episode: 127, duration: 0.389s, episode steps: 166, steps per second: 427, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.365 [-2.738, 0.970], loss: 2.976168, mean_absolute_error: 35.697968, mean_q: 72.083511\
 13423/50000: episode: 128, duration: 0.474s, episode steps: 200, steps per second: 422, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.295 [-2.718, 0.847], loss: 2.354213, mean_absolute_error: 35.596687, mean_q: 72.073837\
 13601/50000: episode: 129, duration: 0.452s, episode steps: 178, steps per second: 394, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.337 [-2.611, 1.033], loss: 3.534891, mean_absolute_error: 36.126240, mean_q: 72.947960\
 13801/50000: episode: 130, duration: 0.533s, episode steps: 200, steps per second: 375, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.261 [-2.617, 1.121], loss: 3.263770, mean_absolute_error: 35.973732, mean_q: 72.538368\
 13946/50000: episode: 131, duration: 0.367s, episode steps: 145, steps per second: 395, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.408 [-2.789, 0.958], loss: 3.520302, mean_absolute_error: 35.974674, mean_q: 72.581551\
 14146/50000: episode: 132, duration: 0.512s, episode steps: 200, steps per second: 391, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.202 [-1.818, 0.745], loss: 2.834984, mean_absolute_error: 36.067837, mean_q: 72.959984\
 14329/50000: episode: 133, duration: 0.517s, episode steps: 183, steps per second: 354, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.320 [-2.553, 0.842], loss: 2.645635, mean_absolute_error: 36.466244, mean_q: 73.762703\
 14483/50000: episode: 134, duration: 0.442s, episode steps: 154, steps per second: 348, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.396 [-2.444, 0.909], loss: 2.256276, mean_absolute_error: 36.205463, mean_q: 73.144264\
 14641/50000: episode: 135, duration: 0.373s, episode steps: 158, steps per second: 424, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.406 [-2.427, 1.026], loss: 2.686020, mean_absolute_error: 36.569687, mean_q: 73.920776\
 14815/50000: episode: 136, duration: 0.457s, episode steps: 174, steps per second: 380, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.346 [-2.629, 0.917], loss: 2.939511, mean_absolute_error: 36.526428, mean_q: 73.755852\
 14979/50000: episode: 137, duration: 0.423s, episode steps: 164, steps per second: 387, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.374 [-2.431, 0.740], loss: 2.516989, mean_absolute_error: 36.050884, mean_q: 72.977272\
 15135/50000: episode: 138, duration: 0.377s, episode steps: 156, steps per second: 413, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.384 [-3.133, 1.420], loss: 2.424936, mean_absolute_error: 36.422878, mean_q: 73.685234\
 15310/50000: episode: 139, duration: 0.444s, episode steps: 175, steps per second: 394, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.340 [-3.011, 1.286], loss: 2.592895, mean_absolute_error: 36.729939, mean_q: 74.219666\
 15453/50000: episode: 140, duration: 0.406s, episode steps: 143, steps per second: 352, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.413 [-2.771, 1.045], loss: 2.672482, mean_absolute_error: 36.536331, mean_q: 73.769707\
 15636/50000: episode: 141, duration: 0.554s, episode steps: 183, steps per second: 330, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.327 [-2.800, 1.149], loss: 3.711480, mean_absolute_error: 36.634930, mean_q: 73.948730\
 15785/50000: episode: 142, duration: 0.362s, episode steps: 149, steps per second: 411, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.392 [-2.598, 0.852], loss: 2.463894, mean_absolute_error: 36.218132, mean_q: 73.012756\
 15974/50000: episode: 143, duration: 0.516s, episode steps: 189, steps per second: 366, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.361 [-0.814, 2.402], loss: 2.682043, mean_absolute_error: 36.139637, mean_q: 73.015587\
 16143/50000: episode: 144, duration: 0.427s, episode steps: 169, steps per second: 396, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.358 [-2.983, 1.155], loss: 2.261570, mean_absolute_error: 36.300789, mean_q: 73.285271\
 16343/50000: episode: 145, duration: 0.518s, episode steps: 200, steps per second: 386, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.302 [-2.931, 0.963], loss: 2.385322, mean_absolute_error: 36.380074, mean_q: 73.463036\
 16487/50000: episode: 146, duration: 0.357s, episode steps: 144, steps per second: 404, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.408 [-2.800, 1.128], loss: 2.713049, mean_absolute_error: 36.438065, mean_q: 73.433556\
 16636/50000: episode: 147, duration: 0.388s, episode steps: 149, steps per second: 384, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.400 [-3.187, 1.352], loss: 1.872933, mean_absolute_error: 36.180809, mean_q: 72.856529\
 16836/50000: episode: 148, duration: 0.507s, episode steps: 200, steps per second: 395, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.166 [-1.857, 0.919], loss: 1.635264, mean_absolute_error: 36.153595, mean_q: 72.976616\
 16992/50000: episode: 149, duration: 0.380s, episode steps: 156, steps per second: 411, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.380 [-2.951, 0.992], loss: 2.090558, mean_absolute_error: 36.351387, mean_q: 73.349800\
 17138/50000: episode: 150, duration: 0.370s, episode steps: 146, steps per second: 395, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.403 [-2.607, 1.114], loss: 2.620050, mean_absolute_error: 36.556007, mean_q: 73.700157\
 17318/50000: episode: 151, duration: 0.479s, episode steps: 180, steps per second: 376, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.345 [-2.759, 0.855], loss: 2.360801, mean_absolute_error: 36.218193, mean_q: 72.958008\
 17479/50000: episode: 152, duration: 0.418s, episode steps: 161, steps per second: 386, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.373 [-2.810, 0.913], loss: 1.960473, mean_absolute_error: 36.687744, mean_q: 73.856430\
 17664/50000: episode: 153, duration: 0.541s, episode steps: 185, steps per second: 342, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.328 [-2.572, 0.877], loss: 2.179395, mean_absolute_error: 36.465824, mean_q: 73.480164\
 17861/50000: episode: 154, duration: 0.532s, episode steps: 197, steps per second: 371, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.310 [-2.436, 0.871], loss: 2.726818, mean_absolute_error: 36.462143, mean_q: 73.508553\
 18061/50000: episode: 155, duration: 0.484s, episode steps: 200, steps per second: 414, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.256 [-2.193, 0.850], loss: 1.916335, mean_absolute_error: 36.164322, mean_q: 73.031097\
 18221/50000: episode: 156, duration: 0.390s, episode steps: 160, steps per second: 411, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.369 [-2.609, 0.988], loss: 1.232347, mean_absolute_error: 36.471077, mean_q: 73.644493\
 18395/50000: episode: 157, duration: 0.465s, episode steps: 174, steps per second: 374, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.358 [-2.781, 0.860], loss: 2.514826, mean_absolute_error: 36.610317, mean_q: 73.778313\
 18556/50000: episode: 158, duration: 0.462s, episode steps: 161, steps per second: 348, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.388 [-2.435, 1.026], loss: 2.263118, mean_absolute_error: 36.251064, mean_q: 73.176331\
 18740/50000: episode: 159, duration: 0.479s, episode steps: 184, steps per second: 384, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.321 [-2.753, 1.224], loss: 2.488231, mean_absolute_error: 35.963051, mean_q: 72.546654\
 18890/50000: episode: 160, duration: 0.358s, episode steps: 150, steps per second: 418, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.410 [-3.095, 1.202], loss: 2.520693, mean_absolute_error: 36.281387, mean_q: 73.157722\
 19090/50000: episode: 161, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.057 [-1.318, 0.881], loss: 1.681860, mean_absolute_error: 36.234066, mean_q: 73.125069\
 19290/50000: episode: 162, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.234 [-2.002, 0.899], loss: 1.956175, mean_absolute_error: 36.219418, mean_q: 73.099510\
 19467/50000: episode: 163, duration: 0.443s, episode steps: 177, steps per second: 400, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.351 [-2.436, 0.806], loss: 2.296129, mean_absolute_error: 36.417439, mean_q: 73.356117\
 19667/50000: episode: 164, duration: 0.568s, episode steps: 200, steps per second: 352, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.206 [-1.818, 1.006], loss: 3.086864, mean_absolute_error: 35.938599, mean_q: 72.391151\
 19818/50000: episode: 165, duration: 0.384s, episode steps: 151, steps per second: 393, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.397 [-2.433, 1.003], loss: 1.722510, mean_absolute_error: 36.477119, mean_q: 73.541817\
 20009/50000: episode: 166, duration: 0.485s, episode steps: 191, steps per second: 394, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.326 [-2.619, 0.851], loss: 2.326654, mean_absolute_error: 36.037502, mean_q: 72.721611\
 20190/50000: episode: 167, duration: 0.460s, episode steps: 181, steps per second: 394, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.347 [-2.947, 1.094], loss: 1.476172, mean_absolute_error: 36.054630, mean_q: 72.809479\
 20390/50000: episode: 168, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.289 [-2.359, 0.995], loss: 1.275694, mean_absolute_error: 35.882984, mean_q: 72.379974\
 20553/50000: episode: 169, duration: 0.392s, episode steps: 163, steps per second: 416, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.378 [-2.409, 0.975], loss: 1.580962, mean_absolute_error: 35.936852, mean_q: 72.492126\
 20747/50000: episode: 170, duration: 0.472s, episode steps: 194, steps per second: 411, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.315 [-3.134, 1.396], loss: 2.726299, mean_absolute_error: 35.744518, mean_q: 72.008858\
 20929/50000: episode: 171, duration: 0.488s, episode steps: 182, steps per second: 373, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.346 [-2.998, 1.126], loss: 2.083458, mean_absolute_error: 35.623009, mean_q: 71.770081\
 21110/50000: episode: 172, duration: 0.555s, episode steps: 181, steps per second: 326, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.324 [-2.738, 1.093], loss: 2.552132, mean_absolute_error: 35.501717, mean_q: 71.453911\
 21302/50000: episode: 173, duration: 0.503s, episode steps: 192, steps per second: 381, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.313 [-2.540, 0.889], loss: 1.535061, mean_absolute_error: 35.337299, mean_q: 71.329689\
 21465/50000: episode: 174, duration: 0.394s, episode steps: 163, steps per second: 414, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.374 [-2.431, 0.990], loss: 1.596179, mean_absolute_error: 35.191559, mean_q: 70.878700\
 21665/50000: episode: 175, duration: 0.500s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.247 [-1.987, 0.815], loss: 2.405273, mean_absolute_error: 35.115067, mean_q: 70.719437\
 21819/50000: episode: 176, duration: 0.368s, episode steps: 154, steps per second: 419, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.393 [-3.182, 1.489], loss: 1.809051, mean_absolute_error: 34.921623, mean_q: 70.459358\
 22019/50000: episode: 177, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.273 [-2.373, 0.958], loss: 2.597041, mean_absolute_error: 34.971188, mean_q: 70.590225\
 22219/50000: episode: 178, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.075 [-1.138, 1.079], loss: 2.106006, mean_absolute_error: 35.397202, mean_q: 71.355736\
 22419/50000: episode: 179, duration: 0.489s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.035 [-1.056, 0.925], loss: 2.454470, mean_absolute_error: 35.206894, mean_q: 70.897774\
 22586/50000: episode: 180, duration: 0.398s, episode steps: 167, steps per second: 420, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.354 [-2.568, 1.093], loss: 1.758814, mean_absolute_error: 35.132004, mean_q: 70.671501\
 22786/50000: episode: 181, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.044 [-1.293, 1.102], loss: 2.627787, mean_absolute_error: 34.993702, mean_q: 70.453674\
 22949/50000: episode: 182, duration: 0.397s, episode steps: 163, steps per second: 410, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.356 [-2.165, 1.205], loss: 2.207937, mean_absolute_error: 35.305477, mean_q: 71.063408\
 23123/50000: episode: 183, duration: 0.425s, episode steps: 174, steps per second: 410, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.358 [-2.445, 0.957], loss: 1.925948, mean_absolute_error: 35.200432, mean_q: 70.950127\
 23281/50000: episode: 184, duration: 0.380s, episode steps: 158, steps per second: 416, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.391 [-2.622, 1.118], loss: 2.205642, mean_absolute_error: 34.940861, mean_q: 70.382637\
 23481/50000: episode: 185, duration: 0.491s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.228 [-1.997, 0.858], loss: 2.034409, mean_absolute_error: 34.899158, mean_q: 70.376976\
 23667/50000: episode: 186, duration: 0.484s, episode steps: 186, steps per second: 384, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.344 [-2.957, 1.015], loss: 1.638917, mean_absolute_error: 34.784130, mean_q: 70.138924\
 23829/50000: episode: 187, duration: 0.390s, episode steps: 162, steps per second: 415, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.381 [-2.398, 1.000], loss: 1.638067, mean_absolute_error: 34.914215, mean_q: 70.344543\
 24029/50000: episode: 188, duration: 0.481s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.156 [-1.700, 0.821], loss: 1.690761, mean_absolute_error: 34.910229, mean_q: 70.428329\
 24229/50000: episode: 189, duration: 0.478s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.101 [-1.888, 1.146], loss: 2.750623, mean_absolute_error: 34.926502, mean_q: 70.287247\
 24404/50000: episode: 190, duration: 0.424s, episode steps: 175, steps per second: 412, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.373 [-2.732, 0.928], loss: 2.154429, mean_absolute_error: 35.193802, mean_q: 70.790131\
 24604/50000: episode: 191, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.062 [-1.180, 0.900], loss: 2.765517, mean_absolute_error: 34.841625, mean_q: 70.159866\
 24804/50000: episode: 192, duration: 0.478s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.242 [-2.011, 1.255], loss: 1.655852, mean_absolute_error: 34.973007, mean_q: 70.522835\
 25004/50000: episode: 193, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.303 [-2.788, 0.942], loss: 2.437861, mean_absolute_error: 34.821720, mean_q: 70.103920\
 25204/50000: episode: 194, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.293 [-2.191, 0.881], loss: 1.445043, mean_absolute_error: 35.021923, mean_q: 70.485802\
 25404/50000: episode: 195, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.099 [-1.269, 0.947], loss: 2.210068, mean_absolute_error: 34.742565, mean_q: 70.002151\
 25604/50000: episode: 196, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.141 [-2.005, 1.162], loss: 1.340885, mean_absolute_error: 35.116787, mean_q: 70.886017\
 25804/50000: episode: 197, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.132 [-1.703, 1.111], loss: 2.166324, mean_absolute_error: 34.995331, mean_q: 70.450462\
 26004/50000: episode: 198, duration: 0.485s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.032 [-1.134, 0.823], loss: 1.685291, mean_absolute_error: 34.872391, mean_q: 70.213654\
 26204/50000: episode: 199, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.040 [-1.216, 0.862], loss: 2.134411, mean_absolute_error: 35.500153, mean_q: 71.494102\
 26404/50000: episode: 200, duration: 0.482s, episode steps: 200, steps per second: 415, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.125 [-1.152, 0.818], loss: 2.087048, mean_absolute_error: 35.290142, mean_q: 71.064522\
 26604/50000: episode: 201, duration: 0.485s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.918, 0.819], loss: 2.689498, mean_absolute_error: 35.554630, mean_q: 71.573669\
 26804/50000: episode: 202, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-1.041, 0.909], loss: 2.421376, mean_absolute_error: 35.861771, mean_q: 72.204590\
 27004/50000: episode: 203, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.030 [-1.089, 1.020], loss: 1.908803, mean_absolute_error: 35.783363, mean_q: 72.166283\
 27204/50000: episode: 204, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.098 [-0.836, 0.876], loss: 3.099888, mean_absolute_error: 36.038593, mean_q: 72.600594\
 27404/50000: episode: 205, duration: 0.492s, episode steps: 200, steps per second: 406, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.149 [-1.881, 1.127], loss: 2.896679, mean_absolute_error: 36.112579, mean_q: 72.523651\
 27604/50000: episode: 206, duration: 0.496s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.109 [-0.937, 1.092], loss: 3.669462, mean_absolute_error: 35.964561, mean_q: 72.368690\
 27804/50000: episode: 207, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.099 [-0.955, 0.750], loss: 2.783628, mean_absolute_error: 36.163074, mean_q: 72.746346\
 28004/50000: episode: 208, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.055 [-0.899, 0.806], loss: 2.597838, mean_absolute_error: 36.121754, mean_q: 72.828522\
 28204/50000: episode: 209, duration: 0.485s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.002 [-1.167, 0.982], loss: 2.548982, mean_absolute_error: 36.615791, mean_q: 73.699471\
 28404/50000: episode: 210, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-1.191, 1.169], loss: 3.413074, mean_absolute_error: 36.187901, mean_q: 72.904510\
 28604/50000: episode: 211, duration: 0.484s, episode steps: 200, steps per second: 413, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.012 [-0.829, 0.842], loss: 2.243133, mean_absolute_error: 36.590702, mean_q: 73.794235\
 28804/50000: episode: 212, duration: 0.480s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.132 [-0.881, 1.069], loss: 1.919745, mean_absolute_error: 36.583176, mean_q: 73.708000\
 29004/50000: episode: 213, duration: 0.491s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.927, 1.113], loss: 1.379012, mean_absolute_error: 36.933666, mean_q: 74.413567\
 29204/50000: episode: 214, duration: 0.472s, episode steps: 200, steps per second: 423, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.038 [-1.313, 1.330], loss: 2.808312, mean_absolute_error: 37.423443, mean_q: 75.427704\
 29404/50000: episode: 215, duration: 0.475s, episode steps: 200, steps per second: 421, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.062 [-1.213, 0.979], loss: 2.583189, mean_absolute_error: 36.947983, mean_q: 74.435677\
 29604/50000: episode: 216, duration: 0.485s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.257 [-0.884, 1.896], loss: 3.423425, mean_absolute_error: 37.006115, mean_q: 74.503723\
 29804/50000: episode: 217, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.922, 0.852], loss: 2.649564, mean_absolute_error: 36.779961, mean_q: 74.002800\
 29997/50000: episode: 218, duration: 0.464s, episode steps: 193, steps per second: 416, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.335 [-2.409, 1.155], loss: 4.022676, mean_absolute_error: 36.824223, mean_q: 74.116577\
 30197/50000: episode: 219, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.075 [-1.103, 0.786], loss: 3.133586, mean_absolute_error: 37.205997, mean_q: 74.905708\
 30397/50000: episode: 220, duration: 0.478s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-1.053, 1.144], loss: 6.155117, mean_absolute_error: 37.286625, mean_q: 74.821457\
 30597/50000: episode: 221, duration: 0.474s, episode steps: 200, steps per second: 422, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.204 [-1.900, 1.017], loss: 3.629660, mean_absolute_error: 37.207821, mean_q: 74.769104\
 30797/50000: episode: 222, duration: 0.476s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.025 [-1.093, 0.856], loss: 2.842711, mean_absolute_error: 37.105587, mean_q: 74.640549\
 30997/50000: episode: 223, duration: 0.479s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.040 [-1.346, 0.882], loss: 3.395523, mean_absolute_error: 37.133827, mean_q: 74.622864\
 31197/50000: episode: 224, duration: 0.485s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.173 [-2.243, 1.532], loss: 5.397585, mean_absolute_error: 37.164722, mean_q: 74.560783\
 31371/50000: episode: 225, duration: 0.409s, episode steps: 174, steps per second: 426, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.359 [-2.410, 1.094], loss: 3.311611, mean_absolute_error: 36.951385, mean_q: 74.182854\
 31571/50000: episode: 226, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.006 [-1.106, 1.057], loss: 1.547785, mean_absolute_error: 36.847660, mean_q: 74.175598\
 31771/50000: episode: 227, duration: 0.474s, episode steps: 200, steps per second: 422, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.111 [-1.287, 0.954], loss: 4.451495, mean_absolute_error: 37.155933, mean_q: 74.656296\
 31971/50000: episode: 228, duration: 0.491s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-1.286, 0.992], loss: 2.193647, mean_absolute_error: 37.459206, mean_q: 75.412659\
 32171/50000: episode: 229, duration: 0.476s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.171 [-0.942, 1.499], loss: 5.658628, mean_absolute_error: 37.644577, mean_q: 75.590149\
 32334/50000: episode: 230, duration: 0.391s, episode steps: 163, steps per second: 417, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.106 [-1.095, 0.736], loss: 6.014668, mean_absolute_error: 36.884075, mean_q: 74.009865\
 32534/50000: episode: 231, duration: 0.480s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.127 [-2.109, 1.281], loss: 3.929826, mean_absolute_error: 37.411057, mean_q: 75.156746\
 32734/50000: episode: 232, duration: 0.479s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.327 [-1.021, 2.312], loss: 2.915497, mean_absolute_error: 37.286533, mean_q: 75.067215\
 32848/50000: episode: 233, duration: 0.272s, episode steps: 114, steps per second: 419, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.123 [-1.255, 1.016], loss: 2.684136, mean_absolute_error: 36.823204, mean_q: 73.999611\
 33048/50000: episode: 234, duration: 0.479s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-1.191, 0.949], loss: 4.931696, mean_absolute_error: 37.299759, mean_q: 74.960915\
 33248/50000: episode: 235, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.151 [-2.036, 1.186], loss: 2.827972, mean_absolute_error: 37.335205, mean_q: 75.035332\
 33448/50000: episode: 236, duration: 0.479s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.216 [-0.856, 1.627], loss: 5.511039, mean_absolute_error: 37.197166, mean_q: 74.661247\
 33648/50000: episode: 237, duration: 0.478s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.091 [-0.837, 0.772], loss: 3.851225, mean_absolute_error: 37.043114, mean_q: 74.534241\
 33848/50000: episode: 238, duration: 0.480s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.073 [-1.179, 1.001], loss: 5.701817, mean_absolute_error: 37.125500, mean_q: 74.602776\
 34048/50000: episode: 239, duration: 0.501s, episode steps: 200, steps per second: 400, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.155 [-0.891, 1.273], loss: 2.282781, mean_absolute_error: 36.889507, mean_q: 74.221367\
 34248/50000: episode: 240, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.784, 0.880], loss: 3.663891, mean_absolute_error: 37.023266, mean_q: 74.507492\
 34448/50000: episode: 241, duration: 0.475s, episode steps: 200, steps per second: 421, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-1.031, 0.768], loss: 2.605500, mean_absolute_error: 37.082001, mean_q: 74.688255\
 34648/50000: episode: 242, duration: 0.474s, episode steps: 200, steps per second: 422, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.035 [-1.229, 1.119], loss: 6.805475, mean_absolute_error: 37.098331, mean_q: 74.483620\
 34848/50000: episode: 243, duration: 0.496s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.015 [-1.210, 0.978], loss: 4.278249, mean_absolute_error: 37.013031, mean_q: 74.425331\
 35048/50000: episode: 244, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.088 [-1.147, 0.942], loss: 2.473952, mean_absolute_error: 37.177208, mean_q: 74.793503\
 35248/50000: episode: 245, duration: 0.489s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.001 [-1.098, 1.010], loss: 5.697810, mean_absolute_error: 37.422535, mean_q: 75.176697\
 35448/50000: episode: 246, duration: 0.490s, episode steps: 200, steps per second: 409, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.037 [-1.317, 0.941], loss: 4.521121, mean_absolute_error: 37.363205, mean_q: 75.194061\
 35611/50000: episode: 247, duration: 0.399s, episode steps: 163, steps per second: 409, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.378 [-2.347, 1.300], loss: 7.466309, mean_absolute_error: 37.132210, mean_q: 74.554039\
 35811/50000: episode: 248, duration: 0.484s, episode steps: 200, steps per second: 414, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.114 [-0.972, 0.884], loss: 5.749651, mean_absolute_error: 37.399136, mean_q: 75.113350\
 35973/50000: episode: 249, duration: 0.391s, episode steps: 162, steps per second: 414, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.389 [-2.384, 1.049], loss: 4.509297, mean_absolute_error: 36.896599, mean_q: 74.212822\
 36173/50000: episode: 250, duration: 0.481s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.009 [-1.111, 1.009], loss: 4.649725, mean_absolute_error: 37.629749, mean_q: 75.573975\
 36373/50000: episode: 251, duration: 0.480s, episode steps: 200, steps per second: 416, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.203 [-0.892, 1.438], loss: 4.853814, mean_absolute_error: 37.017403, mean_q: 74.328049\
 36567/50000: episode: 252, duration: 0.466s, episode steps: 194, steps per second: 416, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.315 [-2.233, 1.455], loss: 4.020665, mean_absolute_error: 36.867680, mean_q: 74.174011\
 36767/50000: episode: 253, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.140 [-1.070, 1.162], loss: 5.321558, mean_absolute_error: 37.271812, mean_q: 74.968163\
 36967/50000: episode: 254, duration: 0.480s, episode steps: 200, steps per second: 417, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.128 [-0.552, 1.111], loss: 3.885541, mean_absolute_error: 37.253792, mean_q: 75.009926\
 37167/50000: episode: 255, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.055 [-1.035, 0.798], loss: 3.344727, mean_absolute_error: 37.101166, mean_q: 74.647720\
 37318/50000: episode: 256, duration: 0.360s, episode steps: 151, steps per second: 420, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.401 [-2.723, 1.029], loss: 5.591722, mean_absolute_error: 37.070190, mean_q: 74.564728\
 37518/50000: episode: 257, duration: 0.478s, episode steps: 200, steps per second: 418, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.090 [-1.437, 1.005], loss: 6.686029, mean_absolute_error: 36.632523, mean_q: 73.593376\
 37682/50000: episode: 258, duration: 0.396s, episode steps: 164, steps per second: 414, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.380 [-2.533, 1.142], loss: 6.364684, mean_absolute_error: 36.673248, mean_q: 73.772049\
 37882/50000: episode: 259, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-0.869, 0.891], loss: 3.853829, mean_absolute_error: 36.436630, mean_q: 73.332283\
 38082/50000: episode: 260, duration: 0.502s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.006 [-0.984, 0.964], loss: 2.922509, mean_absolute_error: 36.860748, mean_q: 74.239990\
 38282/50000: episode: 261, duration: 0.477s, episode steps: 200, steps per second: 420, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.148 [-0.758, 1.292], loss: 3.098635, mean_absolute_error: 37.057262, mean_q: 74.525482\
 38482/50000: episode: 262, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.050 [-1.047, 0.870], loss: 4.060745, mean_absolute_error: 36.979397, mean_q: 74.253441\
 38618/50000: episode: 263, duration: 0.340s, episode steps: 136, steps per second: 400, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.121 [-0.957, 0.948], loss: 4.140051, mean_absolute_error: 37.161076, mean_q: 74.639511\
 38818/50000: episode: 264, duration: 0.521s, episode steps: 200, steps per second: 384, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.098 [-1.154, 1.055], loss: 3.173117, mean_absolute_error: 37.085125, mean_q: 74.556549\
 39018/50000: episode: 265, duration: 0.503s, episode steps: 200, steps per second: 397, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.156 [-0.617, 1.084], loss: 1.579979, mean_absolute_error: 36.910782, mean_q: 74.320801\
 39218/50000: episode: 266, duration: 0.486s, episode steps: 200, steps per second: 412, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.110 [-0.775, 0.931], loss: 4.024954, mean_absolute_error: 36.636494, mean_q: 73.575783\
 39332/50000: episode: 267, duration: 0.300s, episode steps: 114, steps per second: 380, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.115 [-1.168, 1.328], loss: 5.435532, mean_absolute_error: 37.034927, mean_q: 74.259720\
 39532/50000: episode: 268, duration: 0.536s, episode steps: 200, steps per second: 373, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.302 [-2.306, 1.055], loss: 1.605800, mean_absolute_error: 37.209530, mean_q: 74.904243\
 39732/50000: episode: 269, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.051, 1.026], loss: 3.132378, mean_absolute_error: 37.115490, mean_q: 74.538437\
 39932/50000: episode: 270, duration: 0.541s, episode steps: 200, steps per second: 370, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.222 [-0.908, 1.502], loss: 4.916106, mean_absolute_error: 37.061878, mean_q: 74.417160\
 40132/50000: episode: 271, duration: 0.525s, episode steps: 200, steps per second: 381, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.079 [-1.177, 0.994], loss: 3.900031, mean_absolute_error: 37.105469, mean_q: 74.534042\
 40332/50000: episode: 272, duration: 0.495s, episode steps: 200, steps per second: 404, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.935, 1.063], loss: 2.739112, mean_absolute_error: 36.974327, mean_q: 74.408386\
 40532/50000: episode: 273, duration: 0.477s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-1.141, 1.076], loss: 5.287271, mean_absolute_error: 37.074394, mean_q: 74.541878\
 40717/50000: episode: 274, duration: 0.463s, episode steps: 185, steps per second: 399, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.342 [-2.442, 1.024], loss: 6.196031, mean_absolute_error: 37.293106, mean_q: 74.842339\
 40917/50000: episode: 275, duration: 0.498s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-1.074, 0.816], loss: 3.477429, mean_absolute_error: 36.852245, mean_q: 74.137161\
 41117/50000: episode: 276, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.017 [-1.098, 1.054], loss: 5.313747, mean_absolute_error: 36.998425, mean_q: 74.280090\
 41317/50000: episode: 277, duration: 0.509s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.149, 1.002], loss: 5.402952, mean_absolute_error: 37.316219, mean_q: 75.007042\
 41517/50000: episode: 278, duration: 0.566s, episode steps: 200, steps per second: 353, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.075 [-1.470, 1.291], loss: 7.203301, mean_absolute_error: 37.217300, mean_q: 74.783577\
 41717/50000: episode: 279, duration: 0.525s, episode steps: 200, steps per second: 381, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-1.059, 1.336], loss: 3.806052, mean_absolute_error: 37.357456, mean_q: 75.206833\
 41917/50000: episode: 280, duration: 0.524s, episode steps: 200, steps per second: 382, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.037 [-0.863, 0.801], loss: 3.255600, mean_absolute_error: 37.255505, mean_q: 75.013954\
 42117/50000: episode: 281, duration: 0.567s, episode steps: 200, steps per second: 353, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-1.054, 1.092], loss: 2.950116, mean_absolute_error: 37.518623, mean_q: 75.505325\
 42317/50000: episode: 282, duration: 0.848s, episode steps: 200, steps per second: 236, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.003 [-1.052, 0.856], loss: 3.078355, mean_absolute_error: 37.804173, mean_q: 75.951302\
 42517/50000: episode: 283, duration: 0.783s, episode steps: 200, steps per second: 256, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.018 [-1.048, 0.839], loss: 6.881781, mean_absolute_error: 37.695778, mean_q: 75.708778\
 42717/50000: episode: 284, duration: 0.487s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-1.066, 1.045], loss: 4.038832, mean_absolute_error: 37.550556, mean_q: 75.561569\
 42917/50000: episode: 285, duration: 0.490s, episode steps: 200, steps per second: 408, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-1.135, 1.058], loss: 5.938874, mean_absolute_error: 37.685795, mean_q: 75.720947\
 43117/50000: episode: 286, duration: 0.561s, episode steps: 200, steps per second: 357, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.015 [-1.077, 1.028], loss: 5.511091, mean_absolute_error: 37.820385, mean_q: 75.953049\
 43317/50000: episode: 287, duration: 0.519s, episode steps: 200, steps per second: 386, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.001 [-1.055, 1.125], loss: 7.026394, mean_absolute_error: 38.121017, mean_q: 76.526413\
 43517/50000: episode: 288, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.013 [-1.025, 1.053], loss: 4.548724, mean_absolute_error: 37.779507, mean_q: 76.017860\
 43717/50000: episode: 289, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.721, 0.738], loss: 7.970623, mean_absolute_error: 38.002411, mean_q: 76.397484\
 43917/50000: episode: 290, duration: 0.716s, episode steps: 200, steps per second: 280, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.029, 0.835], loss: 4.728464, mean_absolute_error: 37.463837, mean_q: 75.386986\
 44117/50000: episode: 291, duration: 0.546s, episode steps: 200, steps per second: 366, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.931, 0.865], loss: 5.516477, mean_absolute_error: 38.021797, mean_q: 76.509758\
 44317/50000: episode: 292, duration: 0.541s, episode steps: 200, steps per second: 370, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.284 [-2.083, 1.206], loss: 5.290608, mean_absolute_error: 37.729404, mean_q: 75.979736\
 44517/50000: episode: 293, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-1.031, 0.866], loss: 4.792445, mean_absolute_error: 37.634651, mean_q: 75.645920\
 44717/50000: episode: 294, duration: 0.585s, episode steps: 200, steps per second: 342, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.064 [-0.751, 0.743], loss: 6.299997, mean_absolute_error: 37.955742, mean_q: 76.192436\
 44886/50000: episode: 295, duration: 0.468s, episode steps: 169, steps per second: 361, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.380 [-2.807, 1.288], loss: 5.661328, mean_absolute_error: 38.050884, mean_q: 76.575027\
 45086/50000: episode: 296, duration: 0.565s, episode steps: 200, steps per second: 354, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.050 [-1.047, 0.964], loss: 5.705646, mean_absolute_error: 37.901890, mean_q: 76.242546\
 45286/50000: episode: 297, duration: 0.527s, episode steps: 200, steps per second: 380, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.012 [-1.128, 0.894], loss: 7.103553, mean_absolute_error: 37.553242, mean_q: 75.489754\
 45486/50000: episode: 298, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.130 [-0.883, 1.173], loss: 7.599038, mean_absolute_error: 37.751781, mean_q: 75.860542\
 45686/50000: episode: 299, duration: 0.601s, episode steps: 200, steps per second: 333, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.757, 0.969], loss: 6.043060, mean_absolute_error: 37.882809, mean_q: 76.189728\
 45886/50000: episode: 300, duration: 0.478s, episode steps: 200, steps per second: 419, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.079 [-0.721, 0.798], loss: 6.204039, mean_absolute_error: 38.077259, mean_q: 76.593697\
 46086/50000: episode: 301, duration: 0.530s, episode steps: 200, steps per second: 377, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-1.216, 0.863], loss: 5.685246, mean_absolute_error: 38.055576, mean_q: 76.484955\
 46281/50000: episode: 302, duration: 0.516s, episode steps: 195, steps per second: 378, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.319 [-2.419, 1.070], loss: 5.713439, mean_absolute_error: 37.313740, mean_q: 74.945244\
 46462/50000: episode: 303, duration: 0.454s, episode steps: 181, steps per second: 398, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.352 [-2.413, 0.926], loss: 5.765983, mean_absolute_error: 37.940205, mean_q: 76.312195\
 46662/50000: episode: 304, duration: 0.533s, episode steps: 200, steps per second: 375, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.133 [-2.011, 1.138], loss: 7.866326, mean_absolute_error: 37.498295, mean_q: 75.314445\
 46862/50000: episode: 305, duration: 0.538s, episode steps: 200, steps per second: 372, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.113 [-1.691, 1.085], loss: 5.369137, mean_absolute_error: 37.873306, mean_q: 76.061371\
 47062/50000: episode: 306, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.284 [-1.033, 1.963], loss: 7.535915, mean_absolute_error: 37.664135, mean_q: 75.595123\
 47262/50000: episode: 307, duration: 0.534s, episode steps: 200, steps per second: 374, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-1.178, 1.011], loss: 4.524363, mean_absolute_error: 37.671749, mean_q: 75.672562\
 47432/50000: episode: 308, duration: 0.432s, episode steps: 170, steps per second: 394, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.365 [-2.406, 1.164], loss: 6.503134, mean_absolute_error: 37.389900, mean_q: 75.023567\
 47632/50000: episode: 309, duration: 0.486s, episode steps: 200, steps per second: 411, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.161 [-0.831, 1.159], loss: 6.070297, mean_absolute_error: 37.394421, mean_q: 74.998787\
 47819/50000: episode: 310, duration: 0.457s, episode steps: 187, steps per second: 409, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.070 [-1.173, 0.909], loss: 6.946117, mean_absolute_error: 37.171856, mean_q: 74.635170\
 48019/50000: episode: 311, duration: 0.492s, episode steps: 200, steps per second: 406, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.061 [-0.878, 0.792], loss: 4.955372, mean_absolute_error: 37.357666, mean_q: 75.064766\
 48219/50000: episode: 312, duration: 0.497s, episode steps: 200, steps per second: 402, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.158 [-1.640, 1.083], loss: 9.692246, mean_absolute_error: 36.921383, mean_q: 74.098419\
 48419/50000: episode: 313, duration: 0.501s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.116 [-1.320, 0.986], loss: 5.387401, mean_absolute_error: 36.950157, mean_q: 74.375099\
 48619/50000: episode: 314, duration: 0.488s, episode steps: 200, steps per second: 410, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.089 [-1.058, 0.992], loss: 5.611753, mean_absolute_error: 37.219994, mean_q: 74.801704\
 48819/50000: episode: 315, duration: 0.497s, episode steps: 200, steps per second: 403, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.211 [-1.578, 1.709], loss: 8.642712, mean_absolute_error: 37.260040, mean_q: 74.701683\
 49019/50000: episode: 316, duration: 0.511s, episode steps: 200, steps per second: 392, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.065 [-0.922, 1.077], loss: 5.825101, mean_absolute_error: 36.950211, mean_q: 74.145302\
 49149/50000: episode: 317, duration: 0.322s, episode steps: 130, steps per second: 403, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.457 [-2.609, 1.198], loss: 4.663023, mean_absolute_error: 37.177425, mean_q: 74.692009\
 49349/50000: episode: 318, duration: 0.492s, episode steps: 200, steps per second: 407, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-1.155, 0.930], loss: 6.481577, mean_absolute_error: 36.906357, mean_q: 74.051620\
 49549/50000: episode: 319, duration: 0.502s, episode steps: 200, steps per second: 399, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-1.089, 1.016], loss: 5.382028, mean_absolute_error: 36.928429, mean_q: 74.071068\
 49715/50000: episode: 320, duration: 0.418s, episode steps: 166, steps per second: 397, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.365 [-2.761, 1.089], loss: 5.654311, mean_absolute_error: 37.009205, mean_q: 74.245308\
 49915/50000: episode: 321, duration: 0.508s, episode steps: 200, steps per second: 393, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.044 [-1.000, 0.853], loss: 5.921222, mean_absolute_error: 36.830116, mean_q: 73.814018\
